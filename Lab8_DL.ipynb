{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6ab8d1f8",
      "metadata": {
        "id": "6ab8d1f8"
      },
      "source": [
        "# Laboratorio 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b06a0721",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b06a0721",
        "outputId": "ade06c58-eb2b-4bc3-8bfa-7ac46ddd1427"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time, copy, os\n",
        "from typing import Dict, Any\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82079914",
      "metadata": {
        "id": "82079914"
      },
      "source": [
        "## 1. Dataset y DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5ca2bca4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ca2bca4",
        "outputId": "a6075aa8-75a1-444b-af1d-e22d35747004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 70.4MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': 50000, 'val': 10000}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "DATA_DIR = './data'\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 2\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_mean, imagenet_std),\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_mean, imagenet_std),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=train_transform)\n",
        "test_dataset = datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "dataset_sizes = {'train': len(train_dataset), 'val': len(test_dataset)}\n",
        "class_names = train_dataset.classes\n",
        "dataset_sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b47230b",
      "metadata": {
        "id": "6b47230b"
      },
      "source": [
        "## 2. Funciones auxiliares\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "299c39e7",
      "metadata": {
        "id": "299c39e7"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, dataloaders, dataset_sizes, device, num_epochs=5, scheduler=None):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_acc': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 20)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            history[f'{phase}_loss'].append(epoch_loss)\n",
        "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a58b6b49",
      "metadata": {
        "id": "a58b6b49"
      },
      "source": [
        "## a) Evaluación sin entrenamiento adicional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b13fb39b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b13fb39b",
        "outputId": "de12d03a-67f9-4cc6-933c-2342d2616a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 528M/528M [00:03<00:00, 155MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline val loss: 2.371482478713989\n",
            "Baseline val acc: 0.1065\n"
          ]
        }
      ],
      "source": [
        "def get_vgg16(num_classes=10, freeze_features=False, pretrained=True):\n",
        "    model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "    if freeze_features:\n",
        "        for p in model.features.parameters():\n",
        "            p.requires_grad = False\n",
        "    in_features = model.classifier[-1].in_features\n",
        "    model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "baseline_model = get_vgg16(num_classes=NUM_CLASSES, freeze_features=False, pretrained=True).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "baseline_model.eval()\n",
        "running_loss = 0.0\n",
        "running_corrects = 0\n",
        "for inputs, labels in dataloaders['val']:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = baseline_model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "baseline_val_loss = running_loss / dataset_sizes['val']\n",
        "baseline_val_acc = running_corrects.double() / dataset_sizes['val']\n",
        "print('Baseline val loss:', baseline_val_loss)\n",
        "print('Baseline val acc:', baseline_val_acc.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d24019",
      "metadata": {
        "id": "29d24019"
      },
      "source": [
        "## b) Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5da1ac97",
      "metadata": {
        "id": "5da1ac97"
      },
      "outputs": [],
      "source": [
        "feature_model = get_vgg16(num_classes=NUM_CLASSES, freeze_features=True, pretrained=True).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "params_to_update = [p for p in feature_model.parameters() if p.requires_grad]\n",
        "optimizer_ft = optim.Adam(params_to_update, lr=1e-3)\n",
        "\n",
        "feature_model, history_feature = train_model(\n",
        "    feature_model,\n",
        "    criterion,\n",
        "    optimizer_ft,\n",
        "    dataloaders,\n",
        "    dataset_sizes,\n",
        "    device,\n",
        "    num_epochs=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bec060b",
      "metadata": {
        "id": "5bec060b"
      },
      "source": [
        "## c) Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9dd5b30",
      "metadata": {
        "id": "a9dd5b30"
      },
      "outputs": [],
      "source": [
        "finetune_model = get_vgg16(num_classes=NUM_CLASSES, freeze_features=False, pretrained=True).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft_all = optim.Adam(finetune_model.parameters(), lr=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer_ft_all, step_size=2, gamma=0.1)\n",
        "\n",
        "finetune_model, history_finetune = train_model(\n",
        "    finetune_model,\n",
        "    criterion,\n",
        "    optimizer_ft_all,\n",
        "    dataloaders,\n",
        "    dataset_sizes,\n",
        "    device,\n",
        "    num_epochs=3,\n",
        "    scheduler=scheduler\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c8bb3c2",
      "metadata": {
        "id": "3c8bb3c2"
      },
      "source": [
        "## Evaluación y comparación de resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b09c60",
      "metadata": {
        "id": "25b09c60"
      },
      "outputs": [],
      "source": [
        "def plot_history(history: Dict[str, Any], title: str):\n",
        "    plt.figure()\n",
        "    plt.plot(history['train_loss'], label='train loss')\n",
        "    plt.plot(history['val_loss'], label='val loss')\n",
        "    plt.title(f'{title} - Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(history['train_acc'], label='train acc')\n",
        "    plt.plot(history['val_acc'], label='val acc')\n",
        "    plt.title(f'{title} - Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history_feature, 'Feature Extraction')\n",
        "plot_history(history_finetune, 'Fine-Tuning')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ff38715",
      "metadata": {
        "id": "0ff38715"
      },
      "source": [
        "### Tabla comparativa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5583764c",
      "metadata": {
        "id": "5583764c"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "results.append({\n",
        "    'escenario': 'a) baseline (sin entrenamiento)',\n",
        "    'train_acc': None,\n",
        "    'train_loss': None,\n",
        "    'val_acc': baseline_val_acc.item(),\n",
        "    'val_loss': baseline_val_loss,\n",
        "})\n",
        "\n",
        "def best_from_history(h):\n",
        "    best_idx = int(torch.tensor(h['val_acc']).argmax().item())\n",
        "    return h['train_acc'][best_idx], h['train_loss'][best_idx], h['val_acc'][best_idx], h['val_loss'][best_idx]\n",
        "\n",
        "fa_tr_acc, fa_tr_loss, fa_val_acc, fa_val_loss = best_from_history(history_feature)\n",
        "results.append({\n",
        "    'escenario': 'b) feature extraction',\n",
        "    'train_acc': fa_tr_acc,\n",
        "    'train_loss': fa_tr_loss,\n",
        "    'val_acc': fa_val_acc,\n",
        "    'val_loss': fa_val_loss,\n",
        "})\n",
        "\n",
        "fi_tr_acc, fi_tr_loss, fi_val_acc, fi_val_loss = best_from_history(history_finetune)\n",
        "results.append({\n",
        "    'escenario': 'c) fine-tuning',\n",
        "    'train_acc': fi_tr_acc,\n",
        "    'train_loss': fi_tr_loss,\n",
        "    'val_acc': fi_val_acc,\n",
        "    'val_loss': fi_val_loss,\n",
        "})\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}